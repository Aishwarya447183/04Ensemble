{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f3245-7a89-4582-ae6c-473099d027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1.\n",
    "To preprocess the dataset for building a random forest classifier, we need to handle missing values, encode categorical variables, and scale numerical features if necessary. Let's go through each step:\n",
    "\n",
    "Handling Missing Values:\n",
    "\n",
    "Load the dataset and check for missing values.\n",
    "If missing values are present, decide on an appropriate strategy to handle them. Common strategies include:\n",
    "Removing instances with missing values.\n",
    "Removing features with a high number of missing values.\n",
    "Imputing missing values with the mean, median, mode, or a more advanced method like K-nearest neighbors (KNN) imputation.\n",
    "Encoding Categorical Variables:\n",
    "\n",
    "Identify the categorical variables in the dataset.\n",
    "Encode them into numerical values suitable for the random forest classifier.\n",
    "Common encoding methods for categorical variables include one-hot encoding and label encoding.\n",
    "Scaling Numerical Features (if necessary):\n",
    "\n",
    "Check if any numerical features in the dataset need scaling.\n",
    "Scaling is often necessary if the numerical features have different scales or units.\n",
    "Common scaling methods include standardization (subtracting the mean and dividing by the standard deviation) or normalization (scaling the values to a specified range, such as [0, 1]).\n",
    "Here's a code example in Python that demonstrates how to preprocess the dataset for a random forest classifier:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_url = 'https://drive.google.com/uc?export=download&id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Handle missing values\n",
    "missing_values = X.isnull().sum()\n",
    "# Determine appropriate strategy to handle missing values\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Replace categorical columns with encoded values\n",
    "X.drop(categorical_cols, axis=1, inplace=True)\n",
    "X = pd.concat([X, pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names_out(categorical_cols))], axis=1)\n",
    "\n",
    "# Scale numerical features (if necessary)\n",
    "numerical_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the random forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "Make sure to install the required dependencies like pandas, scikit-learn, and numpy before running the code.\n",
    "\n",
    "Note: The code assumes that missing values and appropriate strategies have already been determined. You may need to adjust the code based on the specific requirements of your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bddc69-48a5-433a-b14c-8e4b1cc0b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "\n",
    "To split the dataset into a training set and a test set, we can use the train_test_split function from the scikit-learn library. This function allows us to randomly divide the dataset into two portions based on the specified test size. In this case, we'll use a test size of 30%, meaning the training set will contain 70% of the data, and the test set will contain 30% of the data.\n",
    "\n",
    "Here's the code to split the dataset into a training set and a test set\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_url = 'https://drive.google.com/uc?export=download&id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ'\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "Make sure to install the required dependencies like pandas and scikit-learn before running the code.\n",
    "\n",
    "In the code above, we first load the dataset and separate the features (X) from the target variable (y). Then, we use the train_test_split function to split the data into training and testing sets, with a test size of 0.3 (or 30%). The random_state parameter is set to 42 for reproducibility, but you can change it to any desired value.\n",
    "\n",
    "After running the code, you'll see the shapes of the training and testing sets printed, indicating the number of instances and features in each set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb10e5a-d64e-4688-84f1-bcbe7a3169c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "\n",
    "To train a random forest classifier on the given dataset to predict the risk of heart disease, we'll follow these steps:\n",
    "\n",
    "Step 1: Download and Load the Dataset\n",
    "\n",
    "Download the dataset from the provided link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=share_link\n",
    "Once downloaded, load the dataset into your Python environment.\n",
    "Step 2: Preprocess the Dataset\n",
    "\n",
    "Perform any necessary preprocessing steps on the dataset, such as handling missing values, encoding categorical variables, or scaling numerical features. Ensure that the dataset is in the appropriate format for training the random forest classifier.\n",
    "Step 3: Split the Dataset into Training and Testing Sets\n",
    "\n",
    "Split the preprocessed dataset into a training set and a testing set. The training set will be used to train the random forest classifier, while the testing set will be used to evaluate its performance.\n",
    "Step 4: Train the Random Forest Classifier\n",
    "\n",
    "Import the necessary libraries for building the random forest classifier (e.g., scikit-learn).\n",
    "Create an instance of the random forest classifier with the desired hyperparameters.\n",
    "Fit the classifier to the training data using the fit method.\n",
    "Step 5: Evaluate the Classifier\n",
    "\n",
    "Use the trained classifier to make predictions on the testing set.\n",
    "Evaluate the performance of the classifier using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n",
    "Optionally, you can also perform cross-validation or tune hyperparameters to further optimize the model's performance.\n",
    "Here's some sample code that demonstrates the implementation of the above steps:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Training and Testing Sets\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Classifier\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "Make sure to preprocess the dataset as needed and adjust the code according to your specific requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de66833-5ae3-4051-9cbc-894fbc463806",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4.\n",
    "\n",
    "To evaluate the performance of the random forest classifier on the test set using accuracy, precision, recall, and F1-score, we need to compute these metrics based on the predicted labels and the true labels. Here's an updated version of the code that includes the evaluation metrics:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Training and Testing Sets\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Classifier\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "In this code, we calculate the accuracy, precision, recall, and F1-score using scikit-learn's accuracy_score, precision_score, recall_score, and f1_score functions, respectively. Adjust the code as needed and preprocess the dataset according to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055a537-07be-4d21-8610-6e45adbb51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5.\n",
    "\n",
    "To identify the top 5 most important features in predicting heart disease risk using a random forest classifier and visualize the feature importances, you can use the following code:\n",
    "    \n",
    "    import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Features and Target\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "\n",
    "# Step 4: Train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = importances.argsort()[::-1]\n",
    "top_features = feature_names[indices][:5]\n",
    "top_importances = importances[indices][:5]\n",
    "\n",
    "# Step 5: Visualize the Feature Importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(top_importances)), top_importances, tick_label=top_features)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 5 Features Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "In this code, we train a random forest classifier using the entire dataset, extract the feature importances using the feature_importances_ attribute of the trained classifier, and then sort the importances in descending order. We select the top 5 features and their corresponding importances, and visualize them using a bar chart.\n",
    "\n",
    "Make sure to adjust the code as needed and preprocess the dataset according to your specific requirements. Additionally, you may want to customize the plot further to suit your preferences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5625a04-96e9-46a1-9e4e-44335e0ec945",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6.\n",
    "\n",
    "\n",
    "To tune the hyperparameters of the random forest classifier using grid search or random search and evaluate the performance using 5-fold cross-validation, you can use the following code as a starting point:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Features and Target\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "\n",
    "# Step 4: Define the Hyperparameter Grid for Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Step 5: Perform Grid Search or Random Search\n",
    "# Grid Search\n",
    "\n",
    "\n",
    "# classifier = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "# grid_search.fit(X, y)\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# Random Search\n",
    "classifier = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(classifier, param_grid, cv=5)\n",
    "random_search.fit(X, y)\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Step 6: Evaluate the Best Model using Cross-Validation\n",
    "best_classifier = RandomForestClassifier(**best_params)\n",
    "cv_scores = cross_val_score(best_classifier, X, y, cv=5)\n",
    "mean_cv_score = cv_scores.mean()\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Cross-Validation Score:\", mean_cv_score)\n",
    "\n",
    "\n",
    "In this code, we first load and preprocess the dataset. Then, we define a parameter grid with different values for the hyperparameters we want to tune. Next, we perform either grid search or random search using GridSearchCV or RandomizedSearchCV, respectively. Finally, we create a random forest classifier with the best parameters found, and evaluate its performance using 5-fold cross-validation.\n",
    "\n",
    "You can uncomment the desired search method (GridSearchCV or RandomizedSearchCV) based on your preference. Adjust the code as needed and preprocess the dataset according to your specific requirements. Additionally, you may want to consider expanding the search space by adding more values or hyperparameters to the parameter grid for a more comprehensive search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8eaeed-61fb-41bc-b012-60a21d893e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7.\n",
    "\n",
    "To report the best set of hyperparameters found by the search and the corresponding performance metrics, as well as compare the performance of the tuned model with the default model, you can use the following code as a continuation of the previous code:\n",
    "\n",
    "\n",
    " import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Features and Target\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "\n",
    "# Step 4: Split the Dataset into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Define the Hyperparameter Grid for Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Step 6: Perform Grid Search or Random Search\n",
    "# Grid Search\n",
    "# classifier = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# Random Search\n",
    "classifier = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(classifier, param_grid, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Step 7: Train and Evaluate the Best Model\n",
    "best_classifier = RandomForestClassifier(**best_params)\n",
    "best_classifier.fit(X_train, y_train)\n",
    "y_pred_best = best_classifier.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best)\n",
    "recall_best = recall_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best)\n",
    "\n",
    "# Step 8: Train and Evaluate the Default Model\n",
    "default_classifier = RandomForestClassifier()\n",
    "default_classifier.fit(X_train, y_train)\n",
    "y_pred_default = default_classifier.predict(X_test)\n",
    "\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "precision_default = precision_score(y_test, y_pred_default)\n",
    "recall_default = recall_score(y_test, y_pred_default)\n",
    "f1_default = f1_score(y_test, y_pred_default)\n",
    "\n",
    "# Step 9: Print the Results\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"\\nPerformance Metrics - Best Model:\")\n",
    "print(\"Accuracy:\", accuracy_best)\n",
    "print(\"Precision:\", precision_best)\n",
    "print(\"Recall:\", recall_best)\n",
    "print(\"F1-score:\", f1_best)\n",
    "\n",
    "print(\"\\nPerformance Metrics - Default Model:\")\n",
    "print(\"Accuracy:\", accuracy_default)\n",
    "print(\"Precision:\", precision_default)\n",
    "print(\"Recall:\", recall_default)\n",
    "print(\"F1-score:\", f1_default)\n",
    "\n",
    "In this code, after splitting the dataset into training and testing sets, we define the hyperparameter grid and perform either grid search \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d7ccd-1c9f-40bc-9d24-ca47134653a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8.\n",
    "\n",
    "To plot the decision boundaries of the random forest classifier on a scatter plot of two of the most important features, we can use the following code:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Download and Load the Dataset\n",
    "dataset_url = \"https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?usp=sharing\"\n",
    "file_id = dataset_url.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "df = pd.read_csv(download_url)\n",
    "\n",
    "# Step 2: Preprocess the Dataset (if needed)\n",
    "\n",
    "# Step 3: Split the Dataset into Features and Target\n",
    "X = df.drop('target', axis=1)  # Assuming 'target' column contains the labels\n",
    "y = df['target']\n",
    "\n",
    "# Step 4: Train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Step 5: Get the Most Important Features\n",
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_features = feature_names[indices][:2]  # Select the top 2 features\n",
    "\n",
    "# Step 6: Prepare Data for Scatter Plot\n",
    "X_top = X[top_features]\n",
    "\n",
    "# Step 7: Create Meshgrid for Decision Boundaries\n",
    "plot_step = 0.02\n",
    "x_min, x_max = X_top.iloc[:, 0].min() - 1, X_top.iloc[:, 0].max() + 1\n",
    "y_min, y_max = X_top.iloc[:, 1].min() - 1, X_top.iloc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "# Step 8: Predict and Plot Decision Boundaries\n",
    "Z = rf_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "# Step 9: Plot the Scatter Plot\n",
    "plt.scatter(X_top.iloc[:, 0], X_top.iloc[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
    "plt.xlabel(top_features[0])\n",
    "plt.ylabel(top_features[1])\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
